{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.log4j.{Level, Logger}\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.feature.{RegexTokenizer, StopWordsRemover}\n",
       "import org.apache.spark.ml.feature.CountVectorizerModel\n",
       "import org.apache.spark.ml.feature.CountVectorizer\n",
       "import org.apache.spark.ml.feature.IDF\n",
       "import org.apache.spark.ml.feature.StringIndexer\n",
       "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// package paristech\n",
    "\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.feature.{RegexTokenizer, StopWordsRemover}\n",
    "import org.apache.spark.ml.feature.CountVectorizerModel\n",
    "import org.apache.spark.ml.feature.CountVectorizer\n",
    "import org.apache.spark.ml.feature.IDF\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "object Trainer {\n",
    "\n",
    "  def main(args: Array[String]): Unit = {\n",
    "\n",
    "    val conf = new SparkConf().setAll(Map(\n",
    "      \"spark.scheduler.mode\" -> \"FIFO\",\n",
    "      \"spark.speculation\" -> \"false\",\n",
    "      \"spark.reducer.maxSizeInFlight\" -> \"48m\",\n",
    "      \"spark.serializer\" -> \"org.apache.spark.serializer.KryoSerializer\",\n",
    "      \"spark.kryoserializer.buffer.max\" -> \"1g\",\n",
    "      \"spark.shuffle.file.buffer\" -> \"32k\",\n",
    "      \"spark.default.parallelism\" -> \"12\",\n",
    "      \"spark.sql.shuffle.partitions\" -> \"12\",\n",
    "      \"spark.driver.maxResultSize\" -> \"2g\"\n",
    "    ))\n",
    "\n",
    "    val spark = SparkSession\n",
    "      .builder\n",
    "      .config(conf)\n",
    "      .appName(\"TP Spark : Trainer\")\n",
    "      .getOrCreate()\n",
    "\n",
    "    val rootLogger = Logger.getRootLogger()\n",
    "    rootLogger.setLevel(Level.ERROR)\n",
    "\n",
    "\n",
    "    /*******************************************************************************\n",
    "      *\n",
    "      *       TP 3\n",
    "      *\n",
    "      *       - lire le fichier sauvegarder précédemment\n",
    "      *       - construire les Stages du pipeline, puis les assembler\n",
    "      *       - trouver les meilleurs hyperparamètres pour l'entraînement du pipeline avec une grid-search\n",
    "      *       - Sauvegarder le pipeline entraîné\n",
    "      *\n",
    "      *       if problems with unimported modules => sbt plugins update\n",
    "      *\n",
    "      ********************************************************************************/\n",
    "\n",
    "    println(\"hello world ! from Trainer\")\n",
    "\n",
    "    // #### Processing ####\n",
    "    // Lecture du fichier cleané du TP précédent\n",
    "    val parquetFileDF = spark.read.parquet(\"TP3_input/prepared_trainingset\")\n",
    "\n",
    "    // Split en mots\n",
    "    val tokenizer = new RegexTokenizer()\n",
    "      .setPattern(\"\\\\W+\")\n",
    "      .setGaps(true)\n",
    "      .setInputCol(\"text\")\n",
    "      .setOutputCol(\"tokens\")\n",
    "\n",
    "    // Retrait des Stop Words\n",
    "    val remover = new StopWordsRemover()\n",
    "      .setInputCol(\"tokens\")\n",
    "      .setOutputCol(\"filtered\")\n",
    "\n",
    "    // Term frequency\n",
    "    val cvModel: CountVectorizer = new CountVectorizer()\n",
    "      .setInputCol(\"filtered\")\n",
    "      .setOutputCol(\"rawfeatures\")\n",
    "\n",
    "    // Inverse document frequency\n",
    "    val idf = new IDF()\n",
    "      .setInputCol(\"rawfeatures\")\n",
    "      .setOutputCol(\"tfidf\")\n",
    "\n",
    "    // country2 en numérique\n",
    "    val indexer = new StringIndexer()\n",
    "      .setInputCol(\"country2\")\n",
    "      .setOutputCol(\"country_indexed\")\n",
    "\n",
    "    // currency 2 en numérique\n",
    "    val indexer2 = new StringIndexer()\n",
    "      .setInputCol(\"currency2\")\n",
    "      .setOutputCol(\"currency_indexed\")\n",
    "\n",
    "    // One-Hot encoding\n",
    "    val encoder = new OneHotEncoderEstimator()\n",
    "      .setInputCols(Array(\"country_indexed\", \"currency_indexed\"))\n",
    "      .setOutputCols(Array(\"country_onehot\", \"currency_onehot\"))\n",
    "\n",
    "    // #### ML ####\n",
    "    // Creation du vecteur features\n",
    "    val assembler = new VectorAssembler()\n",
    "      .setInputCols(Array(\"tfidf\", \"days_campaign\", \"hours_prepa\",\"goal\",\"country_onehot\",\"currency_onehot\"))\n",
    "      .setOutputCol(\"features\")\n",
    "\n",
    "    // Classifieur regression logistique\n",
    "    val lr = new LogisticRegression()\n",
    "      .setElasticNetParam(0.0)\n",
    "      .setFitIntercept(true)\n",
    "      .setFeaturesCol(\"features\")\n",
    "      .setLabelCol(\"final_status\")\n",
    "      .setStandardization(true)\n",
    "      .setPredictionCol(\"predictions\")\n",
    "      .setRawPredictionCol(\"raw_predictions\")\n",
    "      .setThresholds(Array(0.7, 0.3))\n",
    "      .setTol(1.0e-6)\n",
    "      .setMaxIter(20)\n",
    "\n",
    "    // Pipeline\n",
    "    val pipeline = new Pipeline()\n",
    "      .setStages(Array(tokenizer, remover, cvModel,idf , indexer, indexer2, encoder, assembler, lr))\n",
    "\n",
    "    // Split données en training et test\n",
    "    val Array(training,test) = parquetFileDF.randomSplit(Array(0.9, 0.1))\n",
    "\n",
    "    // Entraînement du modèle sur train\n",
    "    val model = pipeline.fit(training)\n",
    "\n",
    "    // Calcul de la prédiction\n",
    "    val dfWithSimplePredictions = model.transform(test)\n",
    "    dfWithSimplePredictions.groupBy(\"final_status\", \"predictions\").count.show()\n",
    "\n",
    "    // Performance du modèle\n",
    "    val evaluator = new MulticlassClassificationEvaluator()\n",
    "      .setLabelCol(\"final_status\")\n",
    "      .setPredictionCol(\"predictions\")\n",
    "      .setMetricName(\"f1\")\n",
    "\n",
    "    // Taux de bonnes prédictions\n",
    "    val accuracy = evaluator.evaluate(dfWithSimplePredictions)\n",
    "    println(s\"Accuracy reg log: ${accuracy}\")\n",
    "\n",
    "    // Recherche d'un meilleur modèle par Gridsearch\n",
    "    val paramGrid = new ParamGridBuilder()\n",
    "      .addGrid(cvModel.minDF, Array(55.0, 75.0, 95.0))\n",
    "      .addGrid(lr.regParam, Array(10e-8, 10e-6, 10e-4, 10e-2))\n",
    "      .addGrid(lr.elasticNetParam,Array(0.0, 0.5, 1.0))\n",
    "      .build()\n",
    "\n",
    "\n",
    "    // Evaluation en train / validation\n",
    "    val trainValidationSplit = new TrainValidationSplit()\n",
    "      .setEstimator(pipeline)\n",
    "      .setEvaluator(evaluator)\n",
    "      .setEstimatorParamMaps(paramGrid)\n",
    "      .setTrainRatio(0.7)\n",
    "      .setParallelism(2)\n",
    "\n",
    "    // fitting du meilleur modèle sur train\n",
    "    val model_best = trainValidationSplit.fit(training)\n",
    "\n",
    "    // prediction sur test\n",
    "    val dfWithSimplePredictions_best = model_best.transform(test)\n",
    "\n",
    "    // affichage\n",
    "    dfWithSimplePredictions_best.select(\"features\", \"final_status\", \"predictions\")\n",
    "      .show()\n",
    "\n",
    "    // accuracy du meilleur modèle\n",
    "    val accuracy_best = evaluator.evaluate(dfWithSimplePredictions_best)\n",
    "    println(s\"Accuracy best reg log: ${accuracy_best}\")\n",
    "\n",
    "\n",
    "\n",
    "    // #### enregistrement du meilleur modèle ####\n",
    "    model.write.overwrite().save(\"TP3_output/spark-log-model\")\n",
    "    model_best.write.overwrite().save(\"TP3_output/spark-log-bestmodel\")\n",
    "\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world ! from Trainer\n",
      "+------------+-----------+-----+\n",
      "|final_status|predictions|count|\n",
      "+------------+-----------+-----+\n",
      "|           1|        0.0| 1798|\n",
      "|           0|        1.0| 2357|\n",
      "|           1|        1.0| 1613|\n",
      "|           0|        0.0| 4987|\n",
      "+------------+-----------+-----+\n",
      "\n",
      "Accuracy reg log: 0.6206550633911129\n",
      "+--------------------+------------+-----------+\n",
      "|            features|final_status|predictions|\n",
      "+--------------------+------------+-----------+\n",
      "|(3856,[0,22,29,71...|           0|        1.0|\n",
      "|(3856,[27,67,84,7...|           0|        0.0|\n",
      "|(3856,[105,192,45...|           0|        0.0|\n",
      "|(3856,[9,89,198,2...|           0|        0.0|\n",
      "|(3856,[5,31,32,42...|           0|        0.0|\n",
      "|(3856,[31,83,171,...|           0|        0.0|\n",
      "|(3856,[13,14,32,5...|           0|        0.0|\n",
      "|(3856,[4,10,33,10...|           0|        0.0|\n",
      "|(3856,[27,28,77,2...|           0|        0.0|\n",
      "|(3856,[16,38,66,6...|           1|        1.0|\n",
      "|(3856,[11,13,55,6...|           0|        1.0|\n",
      "|(3856,[6,125,722,...|           1|        0.0|\n",
      "|(3856,[2,68,132,1...|           1|        0.0|\n",
      "|(3856,[0,84,209,2...|           0|        0.0|\n",
      "|(3856,[93,125,151...|           0|        1.0|\n",
      "|(3856,[9,17,23,23...|           0|        1.0|\n",
      "|(3856,[19,31,122,...|           0|        0.0|\n",
      "|(3856,[5,40,56,65...|           0|        0.0|\n",
      "|(3856,[0,302,470,...|           0|        1.0|\n",
      "|(3856,[4,12,100,1...|           0|        0.0|\n",
      "+--------------------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Accuracy best reg log: 0.6505214598466798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@43f30f03\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@59ebc348\n",
       "rootLogger: org.apache.log4j.Logger = org.apache.log4j.spi.RootLogger@9fc12d8\n",
       "parquetFileDF: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 12 more fields]\n",
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_0643b980f4c6\n",
       "remover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_35cbf84cac45\n",
       "cvModel: org.apache.spark.ml.feature.CountVectorizer = cntVec_717d3e696521\n",
       "idf: org.apache.spark.ml.feature.IDF = idf_5b80d3668cd6\n",
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_3ed3a5bcdaf4\n",
       "indexer2: org.apache.spark.ml.feature.StringIndexer = strIdx_d4e60c4fd7c7\n",
       "encoder: org.apache.spark..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val conf = new SparkConf().setAll(Map(\n",
    "      \"spark.scheduler.mode\" -> \"FIFO\",\n",
    "      \"spark.speculation\" -> \"false\",\n",
    "      \"spark.reducer.maxSizeInFlight\" -> \"48m\",\n",
    "      \"spark.serializer\" -> \"org.apache.spark.serializer.KryoSerializer\",\n",
    "      \"spark.kryoserializer.buffer.max\" -> \"1g\",\n",
    "      \"spark.shuffle.file.buffer\" -> \"32k\",\n",
    "      \"spark.default.parallelism\" -> \"12\",\n",
    "      \"spark.sql.shuffle.partitions\" -> \"12\",\n",
    "      \"spark.driver.maxResultSize\" -> \"2g\"\n",
    "    ))\n",
    "\n",
    "    val spark = SparkSession\n",
    "      .builder\n",
    "      .config(conf)\n",
    "      .appName(\"TP Spark : Trainer\")\n",
    "      .getOrCreate()\n",
    "\n",
    "    val rootLogger = Logger.getRootLogger()\n",
    "    rootLogger.setLevel(Level.ERROR)\n",
    "\n",
    "\n",
    "    /*******************************************************************************\n",
    "      *\n",
    "      *       TP 3\n",
    "      *\n",
    "      *       - lire le fichier sauvegarder précédemment\n",
    "      *       - construire les Stages du pipeline, puis les assembler\n",
    "      *       - trouver les meilleurs hyperparamètres pour l'entraînement du pipeline avec une grid-search\n",
    "      *       - Sauvegarder le pipeline entraîné\n",
    "      *\n",
    "      *       if problems with unimported modules => sbt plugins update\n",
    "      *\n",
    "      ********************************************************************************/\n",
    "\n",
    "    println(\"hello world ! from Trainer\")\n",
    "\n",
    "    // #### Processing ####\n",
    "    // Lecture du fichier cleané du TP précédent\n",
    "    val parquetFileDF = spark.read.parquet(\"TP3_input/prepared_trainingset\")\n",
    "\n",
    "    // Split en mots\n",
    "    val tokenizer = new RegexTokenizer()\n",
    "      .setPattern(\"\\\\W+\")\n",
    "      .setGaps(true)\n",
    "      .setInputCol(\"text\")\n",
    "      .setOutputCol(\"tokens\")\n",
    "\n",
    "    // Retrait des Stop Words\n",
    "    val remover = new StopWordsRemover()\n",
    "      .setInputCol(\"tokens\")\n",
    "      .setOutputCol(\"filtered\")\n",
    "\n",
    "    // Term frequency\n",
    "    val cvModel: CountVectorizer = new CountVectorizer()\n",
    "      .setInputCol(\"filtered\")\n",
    "      .setOutputCol(\"rawfeatures\")\n",
    "\n",
    "    // Inverse document frequency\n",
    "    val idf = new IDF()\n",
    "      .setInputCol(\"rawfeatures\")\n",
    "      .setOutputCol(\"tfidf\")\n",
    "\n",
    "    // country2 en numérique\n",
    "    val indexer = new StringIndexer()\n",
    "      .setInputCol(\"country2\")\n",
    "      .setOutputCol(\"country_indexed\")\n",
    "\n",
    "    // currency 2 en numérique\n",
    "    val indexer2 = new StringIndexer()\n",
    "      .setInputCol(\"currency2\")\n",
    "      .setOutputCol(\"currency_indexed\")\n",
    "\n",
    "    // One-Hot encoding\n",
    "    val encoder = new OneHotEncoderEstimator()\n",
    "      .setInputCols(Array(\"country_indexed\", \"currency_indexed\"))\n",
    "      .setOutputCols(Array(\"country_onehot\", \"currency_onehot\"))\n",
    "\n",
    "    // #### ML ####\n",
    "    // Creation du vecteur features\n",
    "    val assembler = new VectorAssembler()\n",
    "      .setInputCols(Array(\"tfidf\", \"days_campaign\", \"hours_prepa\",\"goal\",\"country_onehot\",\"currency_onehot\"))\n",
    "      .setOutputCol(\"features\")\n",
    "\n",
    "    // Classifieur regression logistique\n",
    "    val lr = new LogisticRegression()\n",
    "      .setElasticNetParam(0.0)\n",
    "      .setFitIntercept(true)\n",
    "      .setFeaturesCol(\"features\")\n",
    "      .setLabelCol(\"final_status\")\n",
    "      .setStandardization(true)\n",
    "      .setPredictionCol(\"predictions\")\n",
    "      .setRawPredictionCol(\"raw_predictions\")\n",
    "      .setThresholds(Array(0.7, 0.3))\n",
    "      .setTol(1.0e-6)\n",
    "      .setMaxIter(20)\n",
    "\n",
    "    // Pipeline\n",
    "    val pipeline = new Pipeline()\n",
    "      .setStages(Array(tokenizer, remover, cvModel,idf , indexer, indexer2, encoder, assembler, lr))\n",
    "\n",
    "    // Split données en training et test\n",
    "    val Array(training,test) = parquetFileDF.randomSplit(Array(0.9, 0.1))\n",
    "\n",
    "    // Entraînement du modèle sur train\n",
    "    val model = pipeline.fit(training)\n",
    "\n",
    "    // Calcul de la prédiction\n",
    "    val dfWithSimplePredictions = model.transform(test)\n",
    "    dfWithSimplePredictions.groupBy(\"final_status\", \"predictions\").count.show()\n",
    "\n",
    "    // Performance du modèle\n",
    "    val evaluator = new MulticlassClassificationEvaluator()\n",
    "      .setLabelCol(\"final_status\")\n",
    "      .setPredictionCol(\"predictions\")\n",
    "      .setMetricName(\"f1\")\n",
    "\n",
    "    // Taux de bonnes prédictions\n",
    "    val accuracy = evaluator.evaluate(dfWithSimplePredictions)\n",
    "    println(s\"Accuracy reg log: ${accuracy}\")\n",
    "\n",
    "    // Recherche d'un meilleur modèle par Gridsearch\n",
    "    val paramGrid = new ParamGridBuilder()\n",
    "      .addGrid(cvModel.minDF, Array(55.0, 75.0, 95.0))\n",
    "      .addGrid(lr.regParam, Array(10e-8, 10e-6, 10e-4, 10e-2))\n",
    "      .addGrid(lr.elasticNetParam,Array(0.0, 0.5, 1.0))\n",
    "      .build()\n",
    "\n",
    "\n",
    "    // Evaluation en train / validation\n",
    "    val trainValidationSplit = new TrainValidationSplit()\n",
    "      .setEstimator(pipeline)\n",
    "      .setEvaluator(evaluator)\n",
    "      .setEstimatorParamMaps(paramGrid)\n",
    "      .setTrainRatio(0.7)\n",
    "      .setParallelism(2)\n",
    "\n",
    "    // fitting du meilleur modèle sur train\n",
    "    val model_best = trainValidationSplit.fit(training)\n",
    "\n",
    "    // prediction sur test\n",
    "    val dfWithSimplePredictions_best = model_best.transform(test)\n",
    "\n",
    "    // affichage\n",
    "    dfWithSimplePredictions_best.select(\"features\", \"final_status\", \"predictions\")\n",
    "      .show()\n",
    "\n",
    "    // accuracy du meilleur modèle\n",
    "    val accuracy_best = evaluator.evaluate(dfWithSimplePredictions_best)\n",
    "    println(s\"Accuracy best reg log: ${accuracy_best}\")\n",
    "\n",
    "\n",
    "\n",
    "    // #### enregistrement du meilleur modèle ####\n",
    "    model.write.overwrite().save(\"TP3_output/spark-log-model\")\n",
    "    model_best.write.overwrite().save(\"TP3_output/spark-log-bestmodel\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
